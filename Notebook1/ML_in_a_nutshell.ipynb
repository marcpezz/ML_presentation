{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b0d4efc-6d56-47f3-971b-5e220921d053",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "Welcome to this **Machine Learning (ML)** tutorial, where we will explore fundamental concepts and algorithms that are widely used in data science. The tutorials are taken  **GeeksforGeeks** and **Python courses**.\n",
    "\n",
    "## What You'll Learn in This Tutorial\n",
    "\n",
    "In this tutorial, we will cover essential concepts and techniques in ML. Below are the key topics we will explore:\n",
    "\n",
    "### 1. **Kernel Ridge Regression (KRR)**  \n",
    "Kernel Ridge Regression is a combination of ridge regression and the kernel trick. It helps handle non-linear relationships in data by transforming it into a higher-dimensional space where linear regression can be applied.\n",
    "\n",
    "### 2. **Decision Trees**  \n",
    "Decision trees are one of the simplest yet powerful algorithms for classification and regression tasks. \n",
    "\n",
    "### 3. **Random Forest**  \n",
    "Random Forest is an ensemble learning method that builds multiple decision trees and combines their results to improve accuracy and avoid overfitting. \n",
    "\n",
    "### 4. **K-Nearest Neighbors (K-NN)**  \n",
    "K-NN is a simple yet intuitive algorithm used for classification and regression. By examining the \"neighbors\" (or closest points) of a data point, K-NN can predict its class or value. \n",
    "\n",
    "### 5. **Principal Component Analysis (PCA)**  \n",
    "PCA is a dimensionality reduction technique that simplifies data while retaining as much variability as possible. It is commonly used for visualizing high-dimensional data and speeding up other machine learning algorithms by reducing the number of features.\n",
    "\n",
    "### 6. **Neural Networks (Single Perceptron)**  \n",
    "Neural networks, particularly a single-layer perceptron, are the building blocks for more advanced deep learning models. We will dive into how a perceptron works, how it makes predictions, and its limitations when dealing with more complex data.\n",
    "\n",
    "### 7. **Evaluation Metrics for Classification**  \n",
    "To assess the performance of classification algorithms, we need to measure how well our model is doing. In this section, we will cover basic evaluation metrics such as **accuracy**, **precision**, **recall**, **F1 score**, and **confusion matrix**. These metrics help us understand the strengths and weaknesses of a model.\n",
    "\n",
    "## About This Tutorial\n",
    "\n",
    "This tutorial has been  designed to guide you through simple example in ML using **Python** and  **scikit-learn**. Youâ€™ll get hands-on  with each algorithm and see how they are implemented.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df722d44-7391-44d1-9964-afab267b4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn matplotlib numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ac7a47-bbd3-4aa1-a621-4208d751b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f48c98-5f0c-4215-979f-c24efdeba09f",
   "metadata": {},
   "source": [
    "# **Kernel Ridge Regression**\n",
    "\n",
    "Tutorial from [GeeksforGeeks](https://www.geeksforgeeks.org/understanding-kernel-ridge-regression-with-sklearn/).\n",
    "\n",
    "Kernel ridge regression (KRR) is a powerful technique for tackling regression problems, particularly when dealing with non-linear relationships between features and the target variable.  This technique allows for the modeling of complex, nonlinear relationships between variables, making it a valuable asset in data analysis.\n",
    "\n",
    "Kernel ridge regression is a variant of ridge regression that uses the **kernel trick** to learn a linear function in a high-dimensional feature space. This allows KRR to handle nonlinear data without the need for explicit transformation into a higher-dimensional space. This is achieved by using a kernel function, which computes the dot product between two vectors in the high-dimensional space based on their original feature space representations.\n",
    "\n",
    "1. **Kernel Functions:** Kernel functions are the core of KRR. They define the similarity between data points in the high-dimensional feature space. Common kernel functions include the radial basis function (RBF), polynomial kernel, and sigmoid kernel. Each kernel function has its own strengths and weaknesses, and the choice of kernel depends on the specific problem and data characteristics.\n",
    "2. **Regularization:** Regularization is a crucial aspect of KRR. It helps prevent overfitting by adding a penalty term to the loss function. The regularization strength is controlled by the alpha parameter, which determines the magnitude of the penalty.\n",
    "3. **Dual Coefficients:** In KRR, the weights are not computed directly. Instead, the dual coefficients are calculated, which are the weights in the high-dimensional feature space. These coefficients are used to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c26cc-4673-4ea3-9074-210064742042",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=200, n_features=1, noise=0.1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "krr = KernelRidge(kernel='rbf', alpha=1.0, gamma=0.1)\n",
    "krr.fit(X_train, y_train)\n",
    "y_pred = krr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "plt.scatter(X, y, color='black', label='Data')\n",
    "plt.scatter(X_test, y_test, color='red', label='KRR Prediction')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Kernel Ridge Regression with RBF Kernel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de44e6-d637-433e-bd85-992f95e4bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "X = 5 * rng.rand(100, 1)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(X.shape[0] // 5))\n",
    "\n",
    "model = KernelRidge(alpha=1.0, kernel='rbf', gamma=0.5)\n",
    "model.fit(X, y)\n",
    "X_plot = np.linspace(0, 5, 1000)[:, None]\n",
    "y_plot = model.predict(X_plot)\n",
    "\n",
    "plt.scatter(X, y, color='black', label='Data')\n",
    "plt.plot(X_plot, y_plot, color='red', label='KRR Prediction')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Kernel Ridge Regression with RBF Kernel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4f490c-eb46-4431-b638-312a6077a24b",
   "metadata": {},
   "source": [
    "# **Decision Tree**\n",
    "\n",
    "Tutorial from [Lecture 31 of Python Course](https://python-course.eu/machine-learning/decision-trees-in-python.php).  \n",
    "\n",
    "We will concentrate on classification in this decision tree. Decision trees are assigned to the information based learning algorithms which use different measures of information gain for learning. We can use decision trees for issues where we have continuous but also categorical input and target features.\n",
    "\n",
    "The main idea of decision trees is to find those descriptive features which contain the most \"information\" regarding the target feature and then split the dataset along the values of these features such that the target feature values for the resulting sub_datasets are as pure as possible. The descriptive feature which leaves the target feature most purely is said to be the most informative one. This process of finding the \"most informative\" feature is done until we accomplish a stopping criteria where we then finally end up in so called **leaf nodes**.\n",
    "\n",
    "The leaf nodes contain the predictions we will make for new query instances presented to our trained model. This is possible since the model has kind of learned the underlying structure of the training data and hence can, given some assumptions, make predictions about the target feature value (class) of unseen query instances.\n",
    "\n",
    "We use as a practical example a simplified version of the UCI machine learning Zoo Animal Classification dataset which includes properties of animals as descriptive features and the and the animal species as target feature. This dataset consists of 101 rows and 17 categorically valued attributes defining whether an animal has a specific property or not (e.g.hairs, feathers,..). The first attribute represents the name of the animal and will be removed. The target feature consist of 7 integer values [1 to 7] which represents [1:Mammal, 2:Bird, 3:Reptile, 4:Fish, 5:Amphibian, 6:Bug, 7:Invertebrate]\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://python-course.eu/images/machine-learning/Information_Gain_Calculation_600w.webp\" width=\"600\" />\n",
    "</p>\n",
    "\n",
    "The steps to use the sklearn classification decision tree follow the principal sklearn API which are:\n",
    "\n",
    "    1. Choose the model you want to use --> the DecisionTreeClassifier\n",
    "    2. Set the model hyperparameters --> E.g. number of minimum samples per leaf\n",
    "    3. Create a feature data set as well as a target array containing the labels for the instances\n",
    "    4. Fit the model to the training data\n",
    "    5. Use the fitted model on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f2c934-76ee-4b4c-aa5b-dff97d348e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm *.data\n",
    "!wget https://raw.githubusercontent.com/marcpezz/ML_presentation/refs/heads/main/Notebook1/zoo.data\n",
    "!wget https://raw.githubusercontent.com/marcpezz/ML_presentation/refs/heads/main/Notebook1/agaricus-lepiota.data\n",
    "!wget https://raw.githubusercontent.com/marcpezz/ML_presentation/refs/heads/main/Notebook1/wine.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe10cf20-6fad-463c-b83e-83a5723cd666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Import the dataset \n",
    "dataset = pd.read_csv('zoo.data')\n",
    "#We drop the animal names since this is not a good feature to split the data on\n",
    "dataset=dataset.drop('animal_name',axis=1)\n",
    "\n",
    "train_features = dataset.iloc[:80,:-1]\n",
    "test_features = dataset.iloc[80:,:-1]\n",
    "train_targets = dataset.iloc[:80,-1]\n",
    "test_targets = dataset.iloc[80:,-1]\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion = 'entropy').fit(train_features,train_targets)\n",
    "\n",
    "prediction = tree.predict(test_features)\n",
    "\n",
    "\n",
    "print(\"The prediction accuracy is: \",tree.score(test_features,test_targets)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fba4d8-c85b-4c3e-a6f5-591a7fa4741d",
   "metadata": {},
   "source": [
    "# **Random Forest**\n",
    "\n",
    "Tutorial from [Lecture 33 of Python Course](https://python-course.eu/machine-learning/random-forests-in-python.php)\n",
    "\n",
    "Tree models are known to be high variance, low bias models. In consequence, they are prone to overfit the training data. Another drawback of classical tree models is that they are relatively unstable. This instability can lead to the situation that a small change in the composition of the dataset leads to a completely different tree model.\n",
    "\n",
    "For instance, consider the case where a categorically scaled feature *A* is used as the \"root node feature\". Following, this feature is replaced from the dataset an no longer existent in the sub trees. Now imagine the situation where we replace a single row in the dataset and this change leads to the situation that now feature *B* has the largest information gain or reduction in variance respectively. What does that mean? Well, feature *B* is now preferred over feature *A* as \"root node feature\" which leads to a completely different tree just because we have altered one single instance in the dataset. This situation may not only occur at the root node but also at all interior nodes of the tree.\n",
    "\n",
    "The Random Forest approach is based on two concepts, called **bagging** and **subspace sampling**. **Bagging** is the short form for *bootstrap aggregation*. Here we create a multitude of datasets of the same length as the original dataset drawn from the original dataset with replacement (the *bootstrap* in bagging). We then train a tree model for each of the bootstrapped datasets and take the majority prediction of these models for a unseen query instance as our prediction (the *aggregation* in bagging). Here we take the mean or the median for regression tree models and the mode for classification tree models. Bagging advanced us towards our goal of having a more powerful model creating more accurate results. \n",
    "\n",
    "Unfortunately it turns out that the bagged models are correlated and hence often relatively equal. That is, based on the correlation they produce similar results. This can be reduced to the fact that the bagging is using the whole feature set (all the descriptive features) for each model. Now assume that there are one or two very strong features which surmounts all the others  in terms of \"predictiveness\". Even if we change the composition of the data by sampling with replacement, these two are likely to remain the dominant features and hence will serve as the root node or the first hidden node layer respectively. In consequence, the trees look all quite similar. It can done excluding these two feature and hence performing the **subspace sampling**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeb23bd-7d28-408c-88d6-a023800fbe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Load the Zoo dataset (assuming the file is named 'zoo.data')\n",
    "dataset = pd.read_csv('zoo.data', header=None)\n",
    "\n",
    "# Shuffle the dataset\n",
    "dataset = dataset.sample(frac=1)\n",
    "\n",
    "# Assign column names based on the dataset's description\n",
    "dataset.columns = [\n",
    "    'animal_name', 'hair', 'feathers', 'eggs', 'milk', 'airborne', 'aquatic', \n",
    "    'predator', 'toothed', 'backbone', 'breathes', 'venomous', 'fins', \n",
    "    'legs', 'tail', 'domestic', 'catsize', 'type'\n",
    "]\n",
    "\n",
    "# Drop the animal name column as it is not a feature for classification\n",
    "dataset = dataset.drop(['animal_name'], axis=1)\n",
    "\n",
    "# Encode the target and feature values\n",
    "for label in dataset.columns:\n",
    "    dataset[label] = LabelEncoder().fit_transform(dataset[label])\n",
    "\n",
    "# Define features and target variable\n",
    "X = dataset.drop(['type'], axis=1)\n",
    "Y = dataset['type']\n",
    "\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", message=\"The least populated class in y has only 1 members\")\n",
    "\n",
    "# Train and evaluate the model with different numbers of estimators\n",
    "for i in range(1, 100, 5):\n",
    "    Random_Forest_model = RandomForestClassifier(n_estimators=i, criterion=\"entropy\")\n",
    "    accuracy = cross_validate(Random_Forest_model, X, Y, cv=10)['test_score']\n",
    "    print('The accuracy is: ', sum(accuracy) / len(accuracy) * 100, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aed724-ddf7-4921-97b1-bdfbb43d0748",
   "metadata": {},
   "source": [
    "The same, but with the mushroom database from UCI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe59ad3-8bba-4f8f-b4ea-846388177b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "dataset = pd.read_csv('agaricus-lepiota.data',header=None)\n",
    "dataset = dataset.sample(frac=1)\n",
    "dataset.columns = ['target','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment','gill-spacing',\n",
    "             'gill-size','gill-color','stalk-shape','stalk-root','stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring',\n",
    "             'stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type','spore-print-color','population',\n",
    "             'habitat']\n",
    "\n",
    "#Encode the feature values which are strings to integers\n",
    "for label in dataset.columns:\n",
    "    dataset[label] = LabelEncoder().fit(dataset[label]).transform(dataset[label])\n",
    "\n",
    "X = dataset.drop(['target'],axis=1)\n",
    "Y = dataset['target']\n",
    "\n",
    "for i in range(1,5,1):\n",
    "    Random_Forest_model = RandomForestClassifier(n_estimators=i,criterion=\"entropy\")\n",
    "    accuracy = cross_validate(Random_Forest_model,X,Y,cv=10)['test_score']\n",
    "    print('The accuracy is: ',sum(accuracy)/len(accuracy)*100,'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fece8d31-7133-40e4-b1c5-3cebd7fe4ecf",
   "metadata": {},
   "source": [
    "# **k-Nearest Neighbours**\n",
    "\n",
    "Tutorial from [Lecture 8 of Python Course](https://python-course.eu/machine-learning/k-nearest-neighbor-classifier-in-python.php)\n",
    "\n",
    "We will use the \"iris\" dataset provided by the datasets of the sklearn module to test the nearest neighobour classifier. The data set consists of 50 samples from each of three species of Iris\n",
    "\n",
    "    Iris setosa,\n",
    "    Iris virginica and\n",
    "    Iris versicolor.\n",
    "\n",
    "Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres.\n",
    "We calculate the distances between the point of the sample and the object to be classified using the Euclidian distance between two instances. \n",
    "\n",
    "$$d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i -y_i)^2}$$\n",
    "\n",
    "The next step will be to get the **neighbours** list between elements in the dataset. Classification can be computed by a majority **vote** of the nearest neighbors of the unknown sample. Objectss can be classfied by majority vote (not trustable for more distant points). We can solve this problem assign weights to the neighbours (for example using the harmonic series as weights):\n",
    "\n",
    "$$\\sum_{i}^{k}{1/(i+1)} = 1 + \\frac{1}{2} + \\frac{1}{3} + ... + \\frac{1}{k}$$\n",
    "\n",
    "More complex approaches can be used, for example including the real distance as a metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5161132d-11e4-4423-847a-1791febf74fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions used in the set \n",
    "def distance(instance1, instance2):\n",
    "    \"\"\" Calculates the Eucledian distance between two instances\"\"\" \n",
    "    return np.linalg.norm(np.subtract(instance1, instance2))\n",
    "\n",
    "def get_neighbors(training_set, \n",
    "                  labels, \n",
    "                  test_instance, \n",
    "                  k, \n",
    "                  distance):\n",
    "    \"\"\"\n",
    "    get_neighors calculates a list of the k nearest neighbors\n",
    "    of an instance 'test_instance'.\n",
    "    The function returns a list of k 3-tuples.\n",
    "    Each 3-tuples consists of (index, dist, label)\n",
    "    where \n",
    "    index    is the index from the training_set, \n",
    "    dist     is the distance between the test_instance and the \n",
    "             instance training_set[index]\n",
    "    distance is a reference to a function used to calculate the \n",
    "             distances\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for index in range(len(training_set)):\n",
    "        dist = distance(test_instance, training_set[index])\n",
    "        distances.append((training_set[index], dist, labels[index]))\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    neighbors = distances[:k]\n",
    "    return neighbors\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def vote(neighbors):\n",
    "    class_counter = Counter()\n",
    "    for neighbor in neighbors:\n",
    "        class_counter[neighbor[2]] += 1\n",
    "    return class_counter.most_common(1)[0][0]\n",
    "\n",
    "def vote_prob(neighbors):\n",
    "    class_counter = Counter()\n",
    "    for neighbor in neighbors:\n",
    "        class_counter[neighbor[2]] += 1\n",
    "    labels, votes = zip(*class_counter.most_common())\n",
    "    winner = class_counter.most_common(1)[0][0]\n",
    "    votes4winner = class_counter.most_common(1)[0][1]\n",
    "    return winner, votes4winner/sum(votes)\n",
    "\n",
    "def vote_harmonic_weights(neighbors, all_results=True):\n",
    "    class_counter = Counter()\n",
    "    number_of_neighbors = len(neighbors)\n",
    "    for index in range(number_of_neighbors):\n",
    "        class_counter[neighbors[index][2]] += 1/(index+1)\n",
    "    labels, votes = zip(*class_counter.most_common())\n",
    "    #print(labels, votes)\n",
    "    winner = class_counter.most_common(1)[0][0]\n",
    "    votes4winner = class_counter.most_common(1)[0][1]\n",
    "    if all_results:\n",
    "        total = sum(class_counter.values(), 0.0)\n",
    "        for key in class_counter:\n",
    "             class_counter[key] /= total\n",
    "        return winner, class_counter.most_common()\n",
    "    else:\n",
    "        return winner, votes4winner / sum(votes)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5f9ce-4613-4f21-8f33-da4563b73611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset and print some info about it.\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "labels = iris.target\n",
    "\n",
    "for i in [0, 79, 99, 121]:\n",
    "    print(f\"index: {i:3}, features: {data[i]}, label: {labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d3dec-f863-4666-b744-90f9940b857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset between training and testing \n",
    "# seeding is only necessary for the website so that the values are always equal:\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(data))\n",
    "\n",
    "n_test_samples = 12     # number of test samples\n",
    "learn_data = data[indices[:-n_test_samples]]\n",
    "learn_labels = labels[indices[:-n_test_samples]]\n",
    "test_data = data[indices[-n_test_samples:]]\n",
    "test_labels = labels[indices[-n_test_samples:]]\n",
    "\n",
    "print(\"The first samples of our learn set:\")\n",
    "print(f\"{'index':7s}{'data':20s}{'label':3s}\")\n",
    "for i in range(5):\n",
    "    print(f\"{i:4d}   {learn_data[i]}   {learn_labels[i]:3}\")\n",
    "\n",
    "print(\"The first samples of our test set:\")\n",
    "print(f\"{'index':7s}{'data':20s}{'label':3s}\")\n",
    "for i in range(5):\n",
    "    print(f\"{i:4d}   {learn_data[i]}   {learn_labels[i]:3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffec23-6144-4cf5-adee-49b03d628539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the original data \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "colours = (\"r\", \"b\")\n",
    "X = []\n",
    "for iclass in range(3):\n",
    "    X.append([[], [], []])\n",
    "    for i in range(len(learn_data)):\n",
    "        if learn_labels[i] == iclass:\n",
    "            X[iclass][0].append(learn_data[i][0])\n",
    "            X[iclass][1].append(learn_data[i][1])\n",
    "            X[iclass][2].append(sum(learn_data[i][2:]))\n",
    "\n",
    "colours = (\"r\", \"g\", \"y\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for iclass in range(3):\n",
    "       ax.scatter(X[iclass][0], \n",
    "                  X[iclass][1], \n",
    "                  X[iclass][2], \n",
    "                  c=colours[iclass])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5850a51-8a44-42d5-8fe9-35012737fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the distances \n",
    "print(distance([3, 5], [1, 1]))\n",
    "print(distance(learn_data[3], learn_data[44]))\n",
    "\n",
    "#\n",
    "for i in range(n_test_samples):\n",
    "    neighbors = get_neighbors(learn_data, \n",
    "                              learn_labels, \n",
    "                              test_data[i], \n",
    "                              5, \n",
    "                              distance=distance)\n",
    "    print(\"index: \", i, \n",
    "          \", vote_prob: \", vote_prob(neighbors), \n",
    "          \", label: \", test_labels[i], \n",
    "          \", data: \", test_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dacf0d-bbda-4d5d-922f-2be561648d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_test_samples):\n",
    "    neighbors = get_neighbors(learn_data,learn_labels,test_data[i],6,distance=distance)\n",
    "    print(\"index: \", i,\", result of vote: \",vote_harmonic_weights(neighbors,all_results=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dab5e97-2311-48c2-9d4d-fc64a40bc078",
   "metadata": {},
   "source": [
    "# **Principal Component Analysis**\n",
    "\n",
    "Tutorial from [Lecture 35 of Python Course](https://python-course.eu/machine-learning/principal-component-analysis-pca-in-python.php).\n",
    "\n",
    "The principal components of a dataset are the \"directions\" in a dataset which hold the most variation. We want to have this direction (the direction with the largest variance) because in the future we want to use the principal components of the dataset to reduce dimensionality of our dataset the either make it \"plottable\" by reducing it to three or less dimensions, or simply to reduce the size of the dataset without loosing too much of the information.\n",
    "\n",
    "$$var(x)= \\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n}$$\n",
    "\n",
    "We will test the model using the UCI wine dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde5e1b-0d22-4746-89ac-76289a7bc1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "import pandas as pd\n",
    "\n",
    "# Data collection \n",
    "df = pd.read_table('wine.data', sep=',', names=['Alcohol','Malic_acid','Ash','Alcalinity_of_ash','Magnesium','Total_phenols',\n",
    "                                                'Flavanoids','Nonflavanoid_phenols','Proanthocyanins','Color_intensity','Hue',\n",
    "                                                'OD280/OD315_of_diluted_wines','Proline'])\n",
    "\n",
    "print(\"Original Feature Data (First 5 rows):\")\n",
    "print(df.head())\n",
    "\n",
    "# Extracting target labels (assuming the first column in the data file is the target)\n",
    "target1 = df.iloc[:, 0].values  # The first column is assumed to be the class label (target)\n",
    "\n",
    "# Feature data (excluding the target column)\n",
    "features = df.iloc[:, 1:].values\n",
    "\n",
    "# Data normalization (standardizing the features)\n",
    "df1 = StandardScaler().fit_transform(features)\n",
    "\n",
    "# PCA (Reducing to 2 dimensions)\n",
    "PCA_model = PCA(n_components=2, random_state=42)  # Reducing to two components for visualization\n",
    "data_transformed = PCA_model.fit_transform(df1)  # Only features should be passed into PCA\n",
    "\n",
    "# Plot the data\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax0 = fig.add_subplot(111)\n",
    "\n",
    "# Scatter plot for all data points\n",
    "scatter = ax0.scatter(data_transformed[:, 0], data_transformed[:, 1], c=target1, cmap='viridis')\n",
    "\n",
    "# Create a legend (using unique values of the target)\n",
    "# We assume target1 is a categorical variable that represents different classes\n",
    "for l, c in zip(np.unique(target1), ['red', 'green', 'blue']):\n",
    "    ax0.scatter(data_transformed[target1 == l, 0], data_transformed[target1 == l, 1], c=c, label=f'Class {l}')\n",
    "\n",
    "ax0.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48281dc-78be-4c73-a278-232170f32b22",
   "metadata": {},
   "source": [
    "### **Meaning of the plot:**  \n",
    "**X-axis:** Represents the projection of the original data points on the first principal component (PC1).  \n",
    "**Y-axis:** Represents the projection of the original data points on the second principal component (PC2).  \n",
    "**Colors of Points:** Represent the different classes or categories that the samples belong to (e.g., different types of wine in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7218744f-b3fd-4460-8666-0645a4c47e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Explained variance ratio for each principal component:\")\n",
    "print(PCA_model.explained_variance_ratio_)\n",
    "\n",
    "print(\"\\nCumulative explained variance:\")\n",
    "print(np.cumsum(PCA_model.explained_variance_ratio_))\n",
    "\n",
    "print(\"\\nPrincipal components (Eigenvectors):\")\n",
    "print(PCA_model.components_)\n",
    "\n",
    "# Optionally, print the transformed data (first 5 rows)\n",
    "print(\"\\nTransformed data (first 5 rows):\")\n",
    "print(data_transformed[:5, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05aeac5-495d-4679-b0d4-2259abe1a2af",
   "metadata": {},
   "source": [
    "# **A Simple Neural Network from Scratch in Python**\n",
    "\n",
    "Tutorial from [Lecture 12 of Python Courses](https://python-course.eu/machine-learning/simple-neural-network-from-scratch-in-python.php).\n",
    "\n",
    "We will use a neural network with only one perceptron to separate two classes. These classes are *linearly separable*. Two sets of points (or classes) are called linearly separable, if at least one straight line in the plane exists so that all the points of one class are on one side of the line and all the points of the other class are on the other side.\n",
    "\n",
    "$$ \\sum_{i=1}^n x_i \\cdot w_i = 0 $$\n",
    "\n",
    "Otherwise, i.e. if such a decision boundary does not exist, the two classes are called linearly inseparable. In this case, we cannot use a simple neural network.\n",
    "\n",
    "A perceptron with two input values and a bias corresponds to a general straight line. With the aid of the bias value b we can train the perceptron to determine a decision boundary with a non zero intercept *c*.\n",
    "\n",
    "$$ \\sum_{i=1}^n x_i \\cdot w_i  + w_{n+1}\\cdot b= 0 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc3ed2-fb36-4b62-9f15-bd2114bcedaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%writefile perceptrons.py\n",
    " \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 weights,\n",
    "                 bias=1,\n",
    "                 learning_rate=0.3):\n",
    "        \"\"\"\n",
    "        'weights' can be a numpy array, list or a tuple with the\n",
    "        actual values of the weights. The number of input values\n",
    "        is indirectly defined by the length of 'weights'\n",
    "        \"\"\"\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = bias\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    @staticmethod\n",
    "    def unit_step_function(x):\n",
    "        if  x <= 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def __call__(self, in_data):\n",
    "        \"\"\"\n",
    "                Perform forward pass through the perceptron.\n",
    "        \"\"\"\n",
    "        in_data = np.concatenate( (in_data, [self.bias]) )\n",
    "        result = self.weights @ in_data\n",
    "        return Perceptron.unit_step_function(result)\n",
    "    \n",
    "    def adjust(self, \n",
    "               target_result, \n",
    "               in_data):\n",
    "        \"\"\"\n",
    "        Adjust weights based on error.\n",
    "\n",
    "        Args:\n",
    "            target_result: Target output.\n",
    "            in_data: Input data.\n",
    "        \"\"\"\n",
    "        \n",
    "        in_data = np.array(in_data)\n",
    "        calculated_result = self(in_data)\n",
    "        error = target_result - calculated_result\n",
    "        if error != 0:\n",
    "            in_data = np.concatenate( (in_data, [self.bias]) )\n",
    "            correction = error * in_data * self.learning_rate\n",
    "            self.weights += correction\n",
    "            \n",
    "    def evaluate(self, data, labels):\n",
    "        \"\"\"\n",
    "        Evaluate the perceptron on a dataset.\n",
    "\n",
    "        Args:\n",
    "            data: List of input data arrays.\n",
    "            labels: List of corresponding labels (0 or 1).\n",
    "\n",
    "        Returns:\n",
    "            Counter object containing evaluation results.\n",
    "        \"\"\"\n",
    "        evaluation = Counter()\n",
    "        for sample, label in zip(data, labels):\n",
    "            result = self(sample) # predict\n",
    "            if result == label:\n",
    "                evaluation[\"correct\"] += 1\n",
    "            else:\n",
    "                evaluation[\"wrong\"] += 1\n",
    "        return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d182962f-99be-4c32-9df8-12352b224fc9",
   "metadata": {},
   "source": [
    "In our case, we will create an example with linearly separable data sets in 2 dimensions.\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://python-course.eu/images/machine-learning/perceptron_two_inputs_and_bias_400w.webp\" width=\"300\" />\n",
    "</p>\n",
    "\n",
    "$$w_1 \\cdot x_1 + w_2 \\cdot x_2 + w_3 \\cdot b= 0$$\n",
    "$$x_2 = -\\frac{w_1}{w_2} \\cdot x_1 - \\frac{w_3}{w_2} \\cdot b$$\n",
    "$$m = -\\frac{w_1}{w_2}$$\n",
    "$$c = - \\frac{w_3}{w_2} \\cdot b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c0c7b-739c-4e03-80f1-419cc2650909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two 2D clusters and plot them \n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_samples = 1000\n",
    "samples, labels = make_blobs(n_samples=n_samples, \n",
    "                             centers=([2.5, 3], [6.7, 7.9]), \n",
    "                             cluster_std=1.4,random_state=210)\n",
    "\n",
    "colours = ('green', 'magenta', 'blue', 'cyan', 'yellow', 'red')\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for n_class in range(2):\n",
    "    ax.scatter(samples[labels==n_class][:, 0], samples[labels==n_class][:, 1], \n",
    "               c=colours[n_class], s=40, label=str(n_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a58e32-7cf7-41de-ac3e-1fe03b4dc5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset beteen train and test \n",
    "from sklearn.model_selection import train_test_split\n",
    "res = train_test_split(samples, labels, \n",
    "                       train_size=0.8,\n",
    "                       test_size=0.2,\n",
    "                       random_state=1)\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e82a0-b73b-4533-9a3a-7247feef160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use of the in house perceptron to train the model\n",
    "from perceptrons import Perceptron\n",
    "\n",
    "p = Perceptron(weights=[0.3, 0.3, 0.3],\n",
    "               learning_rate=0.8)\n",
    "\n",
    "for sample, label in zip(train_data, train_labels):\n",
    "    p.adjust(label,sample)\n",
    "\n",
    "evaluation = p.evaluate(train_data, train_labels)\n",
    "print(\"Train evaluation:\",evaluation)\n",
    "evaluation = p.evaluate(test_data, test_labels)\n",
    "print(\"Test evaluation:\",evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c6ff9-01ac-42fd-a49b-d57cea9a93f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final results\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plotting learn data\n",
    "colours = ('green', 'blue')\n",
    "for n_class in range(2):\n",
    "    ax.scatter(train_data[train_labels==n_class][:, 0], \n",
    "               train_data[train_labels==n_class][:, 1], \n",
    "               c=colours[n_class], s=40, label=str(n_class))\n",
    "    \n",
    "# plotting test data\n",
    "colours = ('lightgreen', 'lightblue')\n",
    "for n_class in range(2):\n",
    "    ax.scatter(test_data[test_labels==n_class][:, 0], \n",
    "               test_data[test_labels==n_class][:, 1], \n",
    "               c=colours[n_class], s=40, label=str(n_class))\n",
    "\n",
    "\n",
    "    \n",
    "X = np.arange(np.max(samples[:,0]))\n",
    "m = -p.weights[0] / p.weights[1]\n",
    "c = -p.weights[2] / p.weights[1]\n",
    "print(\"m=\",m,\"c=\",c)\n",
    "ax.plot(X, m * X + c )\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dc5cae-b2e9-4460-a89a-236cb7ea97e5",
   "metadata": {},
   "source": [
    "# **Evaluation Metrics**\n",
    "\n",
    "Tutorial from [Lecture 3 of Python Course](https://python-course.eu/machine-learning/evaluation-metrics.php).\n",
    "\n",
    "**Accuracy** is a statistical measure which is defined as the quotient of correct predictions (both True positives (TP) and True negatives (TN)) made by a classifier divided by the sum of all predictions made by the classifier, including False positves (FP) and False negatives (FN). Therefore, the formula for quantifying binary accuracy is:\n",
    "\n",
    "$$\\text{accuracy} = {{TP + TN} \\over {TP + TN + FP + FN}}$$\n",
    "\n",
    "**Precision** is the ratio of the correctly identified positive cases to all the predicted positive cases, i.e. the correctly and the incorrectly cases predicted as positive. Precision is the fraction of retrieved documents that are relevant to the query. The formula:\n",
    "\n",
    "$$precision = {TP \\over {TP + FP}}$$\n",
    "\n",
    "**Recall**, also known as sensitivity, is the ratio of the correctly identified positive cases to all the actual positive cases, which is the sum of the \"False Negatives\" and \"True Positives\".\n",
    "\n",
    "$$recall = {TP \\over {TP + FN}}$$\n",
    "\n",
    "**F$_1$ score:** \n",
    "$$F_1 = {2 \\over {{1 \\over recall} + {1 \\over precision}}} = 2 \\cdot {{{precision}\\cdot{recall}} \\over {{precision} + {recall}} }$$\n",
    "\n",
    "A **confusion matrix**, also called a contingency table or error matrix, is used to visualize the performance of a classifier. The columns of the matrix represent the instances of the predicted classes and the rows represent the instances of the actual class. In the case of binary classification the table has 2 rows and 2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08f43b-df50-4d5d-aa79-68a49048828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, TN, FP, FN = 4, 91, 1, 4\n",
    "\n",
    "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "print(\"accuracy\",accuracy)\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "print(f\"precision: {precision:4.2f}\")\n",
    "\n",
    "recall = TP / (TP + FN)\n",
    "print(f\"recall: {recall:4.2f}\")\n",
    "\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "print(f\"f1_score{f1_score:6.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
